import streamlit as st
import pandas as pd
import numpy as np
import re
from collections import Counter
import jieba
import io
import requests
from snownlp import SnowNLP
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import base64
import tempfile

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="è¯­è¨€åˆ†æå·¥å…·",
    page_icon="ğŸ“š",
    layout="wide"
)

# å¯¼èˆªæ 
st.sidebar.title('å¯¼èˆªæ ')
page = st.sidebar.radio(
    'é€‰æ‹©é¡µé¢',
    ['é¦–é¡µ', 'Bç«™å¼¹å¹•åˆ†æ', 'è¯­æ–™æ¸…æ´—', 'è¯­è¨€åˆ†æ']
)

# é¦–é¡µ
if page == 'é¦–é¡µ':
    st.title('æ¬¢è¿ä½¿ç”¨è¯­è¨€åˆ†æå·¥å…·')
    st.write("""
    æœ¬å·¥å…·æä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š
    - Bç«™å¼¹å¹•åˆ†æï¼šæ”¯æŒBç«™è§†é¢‘å¼¹å¹•è·å–å’Œåˆ†æ
    - è¯­æ–™æ¸…æ´—ï¼šæä¾›å¤šç§æ–‡æœ¬é¢„å¤„ç†é€‰é¡¹
    - è¯­è¨€åˆ†æï¼šåŒ…å«è¯é¢‘ç»Ÿè®¡ã€å­—ç¬¦ç»Ÿè®¡ç­‰åˆ†æåŠŸèƒ½
    
    è¯·ä½¿ç”¨å·¦ä¾§å¯¼èˆªæ é€‰æ‹©æ‰€éœ€åŠŸèƒ½ã€‚
    """)
    
    # å±•ç¤ºä¸€äº›ç¤ºä¾‹æˆ–ä½¿ç”¨è¯´æ˜
    with st.expander("ä½¿ç”¨è¯´æ˜"):
        st.write("""
        1. æ‰€æœ‰åŠŸèƒ½éƒ½æ”¯æŒæ–‡ä»¶å¯¼å…¥å¯¼å‡º
        2. æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼šTXT, CSV, XLSX
        3. å»ºè®®å•æ¬¡å¤„ç†æ–‡æœ¬å¤§å°ä¸è¶…è¿‡10MB
        """)

# Bç«™å¼¹å¹•åˆ†æéƒ¨åˆ†
elif page == 'Bç«™å¼¹å¹•åˆ†æ':
    st.title('Bç«™å¼¹å¹•åˆ†æ')
    
    # è¾“å…¥Bç«™è§†é¢‘URL
    video_url = st.text_input('è¯·è¾“å…¥Bç«™è§†é¢‘URL:')
    
    if video_url:
        try:
            # è·å–è§†é¢‘ä¿¡æ¯
            headers = {
                'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'referer': 'https://www.bilibili.com'
            }
            
            response = requests.get(video_url, headers=headers)
            response.raise_for_status()
            
            # è·å–cid
            cid_match = re.search(r'"cid":(\d+)', response.text)
            if not cid_match:
                st.error("æ— æ³•åœ¨é¡µé¢ä¸­æ‰¾åˆ°cid")
            else:
                cid = cid_match.group(1)
                
                # è·å–å¼¹å¹•æ•°æ®
                danmaku_url = f'https://api.bilibili.com/x/v1/dm/list.so?oid={cid}'
                response = requests.get(danmaku_url, headers=headers)
                content = response.content.decode('utf-8')
                
                pattern = r'<d p="([0-9.]+),.*?">(.*?)</d>'
                matches = re.findall(pattern, content)
                
                if not matches:
                    st.warning("æœªæ‰¾åˆ°å¼¹å¹•")
                else:
                    danmaku_list = []
                    for time_str, text in matches:
                        seconds = float(time_str)
                        minutes = int(seconds // 60)
                        remaining_seconds = int(seconds % 60)
                        formatted_time = f"{minutes:02d}:{remaining_seconds:02d}"
                        
                        danmaku_list.append({
                            'time': formatted_time,
                            'time_seconds': seconds,
                            'text': text.strip()
                        })
                    
                    # æŒ‰æ—¶é—´æ’åº
                    danmaku_list.sort(key=lambda x: x['time_seconds'])
                    
                    # æ˜¾ç¤ºå¼¹å¹•æ•°æ®
                    st.write(f"å…±è·å–åˆ° {len(danmaku_list)} æ¡å¼¹å¹•")
                    
                    # åˆ†æé€‰é¡¹
                    analysis_options = st.multiselect(
                        'é€‰æ‹©åˆ†æç±»å‹',
                        ['è¯é¢‘ç»Ÿè®¡', 'æƒ…æ„Ÿåˆ†æ', 'è¯äº‘å›¾', 'æ—¶é—´åˆ†å¸ƒ']
                    )
                    
                    if analysis_options:
                        texts = [item['text'] for item in danmaku_list]
                        
                        if 'è¯é¢‘ç»Ÿè®¡' in analysis_options:
                            # åˆ†è¯ç»Ÿè®¡
                            words = []
                            for text in texts:
                                words.extend(jieba.cut(text))
                            
                            # è¿‡æ»¤åœç”¨è¯
                            stop_words = {'çš„', 'äº†', 'æ˜¯', 'å•Š', 'å§', 'å—', 'åœ¨', 'å’Œ', 'å°±', 'éƒ½', 'è¿™', 'æœ‰', 'æˆ‘', 'ä½ ', 'ä»–'}
                            words = [w for w in words if w not in stop_words and len(w) > 1]
                            
                            word_freq = Counter(words).most_common(20)
                            freq_df = pd.DataFrame(word_freq, columns=['è¯è¯­', 'é¢‘æ¬¡'])
                            st.write('è¯é¢‘ç»Ÿè®¡ç»“æœ:')
                            st.dataframe(freq_df)
                        
                        if 'æƒ…æ„Ÿåˆ†æ' in analysis_options:
                            sentiments = []
                            for text in texts:
                                try:
                                    sentiment = SnowNLP(text).sentiments
                                    sentiments.append(sentiment)
                                except:
                                    continue
                            
                            sentiment_dist = {
                                'ç§¯æ': len([s for s in sentiments if s > 0.6]),
                                'ä¸­æ€§': len([s for s in sentiments if 0.4 <= s <= 0.6]),
                                'æ¶ˆæ': len([s for s in sentiments if s < 0.4])
                            }
                            
                            st.write('æƒ…æ„Ÿåˆ†æç»“æœ:')
                            sentiment_df = pd.DataFrame([sentiment_dist])
                            st.dataframe(sentiment_df)
                        
                        if 'è¯äº‘å›¾' in analysis_options:
                            # ç”Ÿæˆè¯äº‘
                            word_freq_dict = dict(Counter(words).most_common(100))
                            wc = WordCloud(
                                width=800,
                                height=400,
                                background_color='white',
                                font_path='SimHei.ttf'
                            ).generate_from_frequencies(word_freq_dict)
                            
                            fig, ax = plt.subplots(figsize=(10, 5))
                            ax.imshow(wc, interpolation='bilinear')
                            ax.axis('off')
                            st.pyplot(fig)
                        
                        if 'æ—¶é—´åˆ†å¸ƒ' in analysis_options:
                            times = [item['time_seconds'] for item in danmaku_list]
                            fig, ax = plt.subplots(figsize=(10, 5))
                            ax.hist(times, bins=20)
                            ax.set_xlabel('è§†é¢‘æ—¶é—´(ç§’)')
                            ax.set_ylabel('å¼¹å¹•æ•°é‡')
                            st.pyplot(fig)
                    
                    # å¯¼å‡ºé€‰é¡¹
                    export_format = st.selectbox(
                        'é€‰æ‹©å¯¼å‡ºæ ¼å¼',
                        ['CSV', 'Excel']
                    )
                    
                    if export_format == 'CSV':
                        df = pd.DataFrame(danmaku_list)
                        csv = df.to_csv(index=False).encode('utf-8-sig')
                        st.download_button(
                            label="ä¸‹è½½CSVæ–‡ä»¶",
                            data=csv,
                            file_name="danmaku.csv",
                            mime="text/csv"
                        )
                    else:
                        df = pd.DataFrame(danmaku_list)
                        with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx') as tmp:
                            df.to_excel(tmp.name, index=False)
                            with open(tmp.name, 'rb') as f:
                                excel_data = f.read()
                            st.download_button(
                                label="ä¸‹è½½Excelæ–‡ä»¶",
                                data=excel_data,
                                file_name="danmaku.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                            )
                            
        except Exception as e:
            st.error(f"è·å–å¼¹å¹•å¤±è´¥: {str(e)}")

# è¯­æ–™æ¸…æ´—éƒ¨åˆ†
elif page == 'è¯­æ–™æ¸…æ´—':
    st.title('è¯­æ–™æ¸…æ´—')
    
    # æ–‡ä»¶ä¸Šä¼ 
    uploaded_file = st.file_uploader("ä¸Šä¼ éœ€è¦æ¸…æ´—çš„æ–‡ä»¶", type=['txt', 'csv'])
    
    if uploaded_file is not None:
        text_to_clean = uploaded_file.read().decode()
    else:
        text_to_clean = st.text_area('æˆ–ç›´æ¥è¾“å…¥éœ€è¦æ¸…æ´—çš„æ–‡æœ¬:', height=200)
    
    if text_to_clean:
        # æ¸…æ´—é€‰é¡¹
        col1, col2 = st.columns(2)
        with col1:
            remove_punct = st.checkbox('åˆ é™¤æ ‡ç‚¹ç¬¦å·')
            remove_numbers = st.checkbox('åˆ é™¤æ•°å­—')
        with col2:
            to_lowercase = st.checkbox('è½¬æ¢ä¸ºå°å†™')
            remove_spaces = st.checkbox('åˆ é™¤å¤šä½™ç©ºæ ¼')
        
        if st.button('å¼€å§‹æ¸…æ´—'):
            # æ‰§è¡Œæ¸…æ´—
            if remove_punct:
                text_to_clean = re.sub(r'[^\w\s]', '', text_to_clean)
            if remove_numbers:
                text_to_clean = re.sub(r'\d+', '', text_to_clean)
            if to_lowercase:
                text_to_clean = text_to_clean.lower()
            if remove_spaces:
                text_to_clean = ' '.join(text_to_clean.split())
            
            st.write('æ¸…æ´—åçš„æ–‡æœ¬:')
            st.text_area('ç»“æœ', text_to_clean, height=200)
            
            # å¯¼å‡ºåŠŸèƒ½
            buf = io.BytesIO()
            buf.write(text_to_clean.encode())
            st.download_button(
                label="ä¸‹è½½æ¸…æ´—åçš„æ–‡æœ¬",
                data=buf.getvalue(),
                file_name="cleaned_text.txt",
                mime="text/plain"
            )

# è¯­è¨€åˆ†æéƒ¨åˆ†
else:
    st.title('è¯­è¨€åˆ†æ')
    
    # æ–‡ä»¶ä¸Šä¼ 
    uploaded_file = st.file_uploader("ä¸Šä¼ è¦åˆ†æçš„æ–‡ä»¶", type=['txt', 'csv'])
    
    if uploaded_file is not None:
        analysis_text = uploaded_file.read().decode()
    else:
        analysis_text = st.text_area('æˆ–ç›´æ¥è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬:', height=200)
    
    if analysis_text:
        # åˆ†æé€‰é¡¹
        analysis_type = st.multiselect(
            'é€‰æ‹©åˆ†æç±»å‹',
            ['è¯é¢‘ç»Ÿè®¡', 'å­—ç¬¦ç»Ÿè®¡', 'è¯äº‘å›¾']
        )
        
        if 'è¯é¢‘ç»Ÿè®¡' in analysis_type:
            words = jieba.lcut(analysis_text)
            word_freq = Counter(words)
            freq_df = pd.DataFrame.from_dict(word_freq, orient='index', columns=['é¢‘æ¬¡'])
            freq_df = freq_df.sort_values('é¢‘æ¬¡', ascending=False)
            
            st.write('è¯é¢‘ç»Ÿè®¡ç»“æœ:')
            st.dataframe(freq_df)
            
            # å¯¼å‡ºè¯é¢‘ç»Ÿè®¡ç»“æœ
            csv = freq_df.to_csv().encode()
            st.download_button(
                label="ä¸‹è½½è¯é¢‘ç»Ÿè®¡ç»“æœ",
                data=csv,
                file_name="word_frequency.csv",
                mime="text/csv"
            )
        
        if 'å­—ç¬¦ç»Ÿè®¡' in analysis_type:
            char_count = len(analysis_text)
            word_count = len(analysis_text.split())
            
            st.write(f'å­—ç¬¦æ•°: {char_count}')
            st.write(f'è¯æ•°: {word_count}')
            
            # å¯¼å‡ºç»Ÿè®¡ç»“æœ
            stats_dict = {'å­—ç¬¦æ•°': char_count, 'è¯æ•°': word_count}
            stats_df = pd.DataFrame([stats_dict])
            csv = stats_df.to_csv().encode()
            st.download_button(
                label="ä¸‹è½½ç»Ÿè®¡ç»“æœ",
                data=csv,
                file_name="text_statistics.csv",
                mime="text/csv"
            )
        if 'è¯äº‘å›¾' in analysis_type:
            # ç”Ÿæˆè¯äº‘å›¾
            words = jieba.lcut(analysis_text)
            word_freq = Counter(words)
            
            # ä½¿ç”¨å®‰å…¨çš„è¯äº‘ç”Ÿæˆå‡½æ•°
            try:
                wc = WordCloud(
                    width=800,
                    height=400,
                    background_color='white',
                    max_words=200,
                    max_font_size=100,
                    random_state=42,
                    font_path='SimHei.ttf'
                ).generate_from_frequencies(word_freq)
                
                # å°†è¯äº‘å›¾è½¬æ¢ä¸ºå›¾ç‰‡
                fig, ax = plt.subplots(figsize=(10, 5))
                ax.imshow(wc, interpolation='bilinear')
                ax.axis('off')
                plt.tight_layout(pad=0)
                
                # åœ¨Streamlitä¸­æ˜¾ç¤ºè¯äº‘å›¾
                st.pyplot(fig)
                
                # æä¾›ä¸‹è½½è¯äº‘å›¾çš„é€‰é¡¹
                img = io.BytesIO()
                plt.savefig(img, format='PNG')
                img.seek(0)
                
                st.download_button(
                    label="ä¸‹è½½è¯äº‘å›¾",
                    data=img,
                    file_name="wordcloud.png",
                    mime="image/png"
                )
                plt.close()
            except Exception as e:
                st.error(f"ç”Ÿæˆè¯äº‘å›¾å¤±è´¥: {str(e)}")
